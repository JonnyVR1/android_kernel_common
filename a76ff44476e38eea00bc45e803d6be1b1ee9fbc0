Revision: a76ff44476e38eea00bc45e803d6be1b1ee9fbc0
Patch-set: 2
File: drivers/cpufreq/cpufreq_interactive.c

61
Wed Nov 03 20:17:56 2010 +0000
Author: Mike Chan <1001534@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: AAAA8H//9Sg=
Bytes: 227
80ms feels pretty high to me but your comments say its based off of UI tests

Will this cause the CPU to spend more time running at higher frequencies? Given how bursty UI workload is there might be more room for power savings.

61
Tue Nov 09 00:56:10 2010 +0000
Author: Todd Poynor <1004277@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: AAAA8H//9Sg=
UUID: AAAA8X///+s=
Bytes: 28
Will continue to tweak this.

64
Wed Nov 03 20:17:56 2010 +0000
Author: Mike Chan <1001534@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: AAAA8H//9Sw=
Bytes: 48
IT would be great it this was tuneable from /sys

64
Tue Nov 09 00:56:10 2010 +0000
Author: Todd Poynor <1004277@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: AAAA8H//9Sw=
UUID: AAAA8X///+o=
Bytes: 8
Will do.

213
Wed Nov 03 20:17:56 2010 +0000
Author: Mike Chan <1001534@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: AAAA8H//9Ss=
Bytes: 177
I am curious why we exit early here. If the timer fires in less than 1ms, its too probably too quick for us to determine if we want to scale up, but can we decide to scale down?

213
Tue Nov 09 00:56:10 2010 +0000
Author: Todd Poynor <1004277@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: AAAA8H//9Ss=
UUID: AAAA8X///+Y=
Bytes: 643
The original patch had a check for delta_time == 0 here, and perhaps that can now be restored.  

I'm hoping this case is now rare or non-existent.  Previously it happened rather frequently when CPU A idle exit raced with CPU B scheduling the timer function: the timer was no longer "pending" but hadn't actually run yet, and by the time it reached this point CPU A had started a new timer.  In each case I saw, the two CPUs returned the exact same timestamp from get_cpu_idle_time_us().  This race is defended against by having idle exit no longer start a new timer if the timer function hasn't run since the last idle exit timer was started.

232
Wed Nov 03 20:17:56 2010 +0000
Author: Mike Chan <1001534@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: AAAA8H//9Sk=
Bytes: 533
Do we want to calculate the load since the last cpu speed change?

Correct me if I'm wrong, but in the case where CPU is at max (1ghz) for 10 seconds, then the load suddenly drops, it will take some time before the CPU lowers its speed.

The ondemand governor uses X as a sample period (30-40ms on Nexus One), so actually in this case we keep the cpu running higher for longer burning more power.

It might be better to look at the load for the last X ms, ie the last min_sample_time for that CPU, so the governor ramps down quickly.

232
Tue Nov 09 00:56:10 2010 +0000
Author: Todd Poynor <1004277@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: AAAA8H//9Sk=
UUID: AAAA8X///94=
Bytes: 1008
The original patch would scale up to max if zero idle time since idle exit or possibly scale down based on the load since last frequency change.  This version of the patch scales up (including incrementally up, not necessarily to max) or down based on the higher of the two loads, since idle exit or since last frequency change.

In looking at the debug logs, there were times in which heavy loads were briefly interrupted by periods of lighter activity, and previously these could cause speed to be lowered, only to be quickly scaled back up when the heavy load resumed (if more than min_sample_time had elapsed since scaling up). 

Like most of these changes, it is biased toward keeping speeds high while the load is (or has recently been) high, only scaling down when sustained lighter loads are seen.   And this is influenced by the hardware we're initially testing it on, where squeezing every last drop of battery juice isn't as critical as it is on some other phones.  Maybe more tunables are needed.

