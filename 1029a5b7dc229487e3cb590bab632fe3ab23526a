Revision: 1029a5b7dc229487e3cb590bab632fe3ab23526a
Patch-set: 4
File: net/ipv4/tcp.c

3553:3-3553:15
Wed Nov 11 04:02:35 2015 +0000
Author: Liping Zhang <1086323@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: 3bf75749_bd5e1f51
Bytes: 111
Here you change the lock_sock() to bh_lock_sock() in 
tcp_nuke_addr, so check sock_owned_by_user(sk) is needed.

3553:3-3553:15
Wed Nov 11 04:51:44 2015 +0000
Author: Lorenzo Colitti <1000835@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 3bf75749_bd5e1f51
UUID: 5bfc0b2b_be5e1951
Bytes: 320
No, sock_owned_by_user guaranteed to be true until we call release_sock. bh_lock_sock does not affect the result of sock_owned_by_user(); all it does is:

    #define bh_lock_sock(__sk)      spin_lock(&((__sk)->sk_lock.slock))

You can confirm this by adding:

    BUG_ON(!sock_owned_by_user(sk));

after bh_lock_sock().

3553:3-3553:15
Wed Nov 11 05:14:27 2015 +0000
Author: Liping Zhang <1086323@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 5bfc0b2b_be5e1951
UUID: f61b1659_06dadc41
Bytes: 587
Hello Lorenzo,
If we lack a judgement of sock_owned_by_user here, race condition
will still exist like this:
CPU0                            CPU1
   tcp_nuke_addr {                tcp_close {
     --                             lock_sock
     bh_lock_sock                   --
     tcp_set_state {                tcp_set_state {
       inet_put_port                    inet_put_port
       --                               icsk_bind_hash->num_owners--
       --                               icsk_bind_hash = NULL
       // access NULL pointer here  }
       icsk_bind_hash->num_owners--

3553:3-3553:15
Wed Nov 11 05:30:49 2015 +0000
Author: Lorenzo Colitti <1000835@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: f61b1659_06dadc41
UUID: 5bfc0b2b_3ee0c9bb
Bytes: 533
No, that can't happen. The reason is the lock_sock statement in line 3551.

If tcp_close executes lock_sock first, then tcp_nuke_addr will block on the lock_sock in line 3551 until tcp_close calls release_sock. Once tcp_close calls release_sock, the SOCK_DEAD flag will be set, and tcp_nuke_addr will not call tcp_done.

If tcp_nuke_addr executes lock_sock first, then tcp_close will block on lock_sock until tcp_nuke_addr calls release_sock. By that time, sk->sk_state is set to TCP_CLOSE, and lock_sock will not call tcp_set_state.

3559:4-3559:12
Wed Nov 11 03:04:02 2015 +0000
Author: Liping Zhang <1086323@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: 5bfc0b2b_1e22ade4
Bytes: 187
I think here miss a judgement of sock_owned_by_user, or it will
still race with inet_put_port() in tcp_close(). So code should be
like this:
if (!sock_owned_by_user(sk))
    tcp_done(sk);

3559:4-3559:12
Wed Nov 11 03:26:19 2015 +0000
Author: Lorenzo Colitti <1000835@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 5bfc0b2b_1e22ade4
UUID: 16218a86_84a4067b
Bytes: 998
That doesn't make sense. Here sock_owned_by_user(sk) is guaranteed to be true, because we have called lock_sock.

lock_sock basically does the following:
  - Wait until sock_owned_by_user(sk) is false
  - Lock the socket
  - Set sock_owned_by_user to 1
  - Unlock the socket

I don't think the race you describe can happen. tcp_close calls inet_put port via tcp_set_state, which is only called with the userspace socket lock held. So there are two cases:

Case #1: tcp_nuke_addr calls lock_sock first. In this case tcp_done() will do "tcp_set_state(sk, TCP_CLOSE)". Then when tcp_nuke_addr calls release_sock, tcp_close will run. But now sk->sk_state == TCP_CLOSE, so tcp_close will do "goto adjudge_to_death" and not call inet_put_port.

Case #2: tcp_close calls lock_sock first. In this case tcp_close will call sock_orphan, which sets SOCK_DEAD. Then it calls sock_release. Now tcp_nuke_addr can run. But because SOCK_DEAD is true, tcp_nuke addr will just skip the socket without doing anything.

